{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1 Tokenizer pad_sequence.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gnNfbE-EGFPe","colab_type":"text"},"source":["##Import libraries you need to use"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ArOPfBwyZtln","colab":{"base_uri":"https://localhost:8080/","height":62},"outputId":"48eb3542-a520-45fe-ca74-1c2157d0884f","executionInfo":{"status":"ok","timestamp":1582596598303,"user_tz":-480,"elapsed":2073,"user":{"displayName":"Jeremy Lai","photoUrl":"","userId":"02259994175981005007"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"2LBown_fGOKx","colab_type":"text"},"source":["##Make some sentences and tokenize the vocabularies in the sentences"]},{"cell_type":"code","metadata":{"id":"3H9JVPA0d4t4","colab_type":"code","outputId":"d2991ff3-f029-4d79-9c65-47b07244a112","executionInfo":{"status":"ok","timestamp":1582596598305,"user_tz":-480,"elapsed":2054,"user":{"displayName":"Jeremy Lai","photoUrl":"","userId":"02259994175981005007"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["sentences = [\n","    'I like playing basketball',\n","    'I like playing baseball',\n","    'You like playing basketball',\n","    'Do you think basketball is amazing?'\n","]\n","\n","tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","print(\"\\nWord Index = \" , word_index)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\n","Word Index =  {'<OOV>': 1, 'like': 2, 'playing': 3, 'basketball': 4, 'i': 5, 'you': 6, 'baseball': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"weJaTXtxGhY4","colab_type":"text"},"source":["##Encode the setences with your tokenizer"]},{"cell_type":"code","metadata":{"id":"X0LrCvUSdrm4","colab_type":"code","outputId":"c0b01aae-fc87-4767-d439-aeb0a87ffb7f","executionInfo":{"status":"ok","timestamp":1582596598306,"user_tz":-480,"elapsed":2048,"user":{"displayName":"Jeremy Lai","photoUrl":"","userId":"02259994175981005007"}},"colab":{"base_uri":"https://localhost:8080/","height":151}},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","\n","padded = pad_sequences(sequences, maxlen=6)\n","print(\"\\nSequences = \" , sequences)\n","print(\"\\nPadded Sequences:\")\n","print(padded)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\n","Sequences =  [[5, 2, 3, 4], [5, 2, 3, 7], [6, 2, 3, 4], [8, 6, 9, 4, 10, 11]]\n","\n","Padded Sequences:\n","[[ 0  0  5  2  3  4]\n"," [ 0  0  5  2  3  7]\n"," [ 0  0  6  2  3  4]\n"," [ 8  6  9  4 10 11]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0YBI7EqaGw8C","colab_type":"text"},"source":["##Test other new sentence"]},{"cell_type":"code","metadata":{"id":"XCEoLVApcYMf","colab_type":"code","outputId":"e95043dc-bc97-4731-86fa-759f1bf0709f","executionInfo":{"status":"ok","timestamp":1582596598306,"user_tz":-480,"elapsed":2042,"user":{"displayName":"Jeremy Lai","photoUrl":"","userId":"02259994175981005007"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["# Try with words that the tokenizer wasn't fit to\n","test_data = [\n","    'i really like playing basketball'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(\"\\nTest Sequence = \", test_seq)\n","\n","padded = pad_sequences(test_seq, maxlen=10)\n","print(\"\\nPadded Test Sequence: \")\n","print(padded)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","Test Sequence =  [[5, 1, 2, 3, 4]]\n","\n","Padded Test Sequence: \n","[[0 0 0 0 0 5 1 2 3 4]]\n"],"name":"stdout"}]}]}